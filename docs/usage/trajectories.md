# Trajectories

The [`trajectories/`](https://github.com/princeton-nlp/SWE-agent/tree/main/trajectories) folder is the default location that experiment results (invocations of [`run.py`](cl_tutorial.md)) will be written to.

At a high level, the experiments folder is organized in the following manner:
```
trajectories
â”œâ”€â”€ <user 1> ğŸ‘©â€ğŸ’»
â”‚ â”œâ”€â”€ <experiment 1> ğŸ§ª
â”‚ â”‚ â”œâ”€â”€ all_preds.jsonl
â”‚ â”‚ â”œâ”€â”€ args.yaml
â”‚ â”‚ â”œâ”€â”€ *.html (Webpage Files)
â”‚ â”‚ â””â”€â”€ *.traj (Trajectories)
â”‚ â””â”€â”€ <experiment 2> ğŸ§ª
â”‚   â”œâ”€â”€ all_preds.jsonl
â”‚   â”œâ”€â”€ args.yaml
â”‚   â”œâ”€â”€ *.html (Webpage Files)
â”‚   â””â”€â”€ *.traj (Trajectories)
â”œâ”€â”€ <user 2> ğŸ‘¨â€ğŸ’»
â”‚ â”œâ”€â”€ <experiment 1> ğŸ§ª
â”‚ â”‚ â””â”€â”€ ...
â”‚ â””â”€â”€ <experiment 2> ğŸ§ª
â”‚   â””â”€â”€ ...
...
```
Where every experiment follows the pattern `trajectories/<user name>/<experiment name>`. The `<user name>` is automatically inferred from your system, and the `experiment name` is inferred from the arguments of the `run.py`.

!!! tip "Viewing trajectories"
    We provide a [trajectory viewer](inspector.md) for an easy viewing of trajectories.

## How an Experiment Folder is Generated

Each call to `run.py` produces a single `trajectories/<user name>/<experiment name>` folder containing the following assets:

* `all_preds.jsonl`: A single file containing all of the predictions generated for the experiment (1 prediction per task instance), where each line is formatted as:
```
{
    "instance_id": "<Unique task instance ID>",
    "model_patch": "<.patch file content string>",
    "model_name_or_path": "<Model name here (Inferred from experiment configs)>",
}
```
* `args.yaml`: A summary of the configurations for the experiment run.
* `<instance_id>.traj`: A `.json` formatted file containing the (thought, action, observation) turns generated by SWE-agent towards solving `<instance_id>`.
* `<instance_id>.html`: An `.html` single webpage render of the trajectory, which can be directly opened in the browser for easier viewing of the trajectory.

!!! tip 
    * Evaluation is not completed by `run.py`, it is a separate step (see [benchmarking](benchmarking.md))
    * `all_preds.jsonl` can be referenced directly into `evaluation/run_eval.sh` to run evaluation (see [benchmarking](benchmarking.md))
    * Trajectories can be turned into custom demonstrations for SWE-agent ([more information](../dev/demonstrations.md)).

{% include-markdown "../_footer.md" %}