{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SWE-agent","text":"<p>SWE-agent turns LMs (e.g. GPT-4) into software engineering agents that can fix bugs and issues in real GitHub repositories.</p> <p>On SWE-bench, SWE-agent resolves 12.29% of issues, achieving the state-of-the-art performance on the full test set.</p> <p>We accomplish our results by designing simple LM-centric commands and feedback formats to make it easier for the LM to browse the repository, view, edit and execute code files. We call this an \ud83e\udd16 Agent-Computer Interface (ACI). Read more about it in our paper!</p> <p>SWE-agent is built and maintained by researchers from Princeton University.</p> <p> </p> <p>If you found this work helpful, please consider using the following citation:</p> <pre><code>@misc{yang2024sweagent,\n      title={SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering},\n      author={John Yang and Carlos E. Jimenez and Alexander Wettig and Kilian Lieret and Shunyu Yao and Karthik Narasimhan and Ofir Press},\n      year={2024},\n}\n</code></pre>"},{"location":"#use-swe-agent-as-a-dev-tool","title":"\u2728 Use SWE-agent as a dev tool","text":"<p>We provide a command line tool and a graphical web interface:</p> <p></p>"},{"location":"_footer/","title":"footer","text":"<ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"config/commands/","title":"Command Configuration","text":"<p>In this document, we describe how to implement your own commands for the SWE-agent ACI. To see examples of command implementations, open the <code>.sh</code> and <code>.py</code> files in the <code>config/commands</code> folder.</p>"},{"location":"config/commands/#scaffolding","title":"Scaffolding","text":"<p>Every command subscribes to the following skeleton code.</p> <pre><code># @yaml\n# signature: [command] [argument(s)]\n# docstring: [Brief description of what your command does.]\n# arguments:\n#   [argument 1 name]:\n#       type: [type (i.e. integer, string)]\n#       description: [Brief description of this argument]\n#       required: [true|false]\n#   [argument 2 name]:\n#       ...\n[command]() {\n    # Implementation here\n}\n</code></pre> <ul> <li>If a command takes in arguments, reference them via positional parameters notation (i.e. <code>$1</code>).</li> <li>If there are no arguments, omit the <code>arguments</code> section.</li> <li>The implementation for your command is unconstrained. There are no limitations on the form of the underlying command code.</li> <li>The minimal documentation requirements are <code>signature</code> and <code>docstring</code>.</li> <li>If you'd like multiple commands to make modifications to a similar body of functions, we recommend using global variables.<ul> <li>For instance, in <code>config/commands/default.sh</code>, you'll see we define the <code>CURRENT_LINE</code> variable for the file viewer. This variable is modified across multiple commands, including <code>open</code>, <code>goto</code>, <code>scroll_up</code>, <code>scroll_down</code>, and <code>edit</code>.</li> <li>You can also leverage third party libraries (check out how we do linting enabled <code>edit</code> in <code>config/commands/edit_linting.sh</code>).</li> </ul> </li> <li>To show effects of the command, print to standard output (i.e. <code>echo</code>). SWE-agent is implemented such that it does not look for a return value from these commands.</li> </ul>"},{"location":"config/commands/#displaying-the-command-to-swe-agent","title":"Displaying the Command to SWE-agent","text":"<p>After you define a command, there are a small set of additional steps to making it available for the agent to use.</p> <p>First, within your config file...</p> <ul> <li>Add <code>config/commands/&lt;file name&gt;.sh</code> file to the <code>command_files</code> field.</li> <li>Set the <code>parse_command</code> field to <code>ParseCommandBash</code> or <code>ParseCommandDetailed</code>. This key points to the functionality that generates how command documentation is shown to the agent.</li> <li>Decide which template(s) you want to show the <code>{command_docs}</code> in.<ul> <li>We strongly recommend including <code>{command_docs}</code> in the <code>system_template</code>, which is the first message shown to the agent for every task instance episode.</li> <li>You might also consider adding <code>{command_docs}</code> to the <code>format_error_template</code>, which is shown if the response provided by a model is malformed.</li> </ul> </li> <li>(Optional) Including a demonstration that uses a command is helpful to showcase proper use + increases the frequency with which the agent uses the command. If you'd like to add a demonstration...<ul> <li>Create a demonstration manually (i.e. <code>python run.py --model human_thought ...</code>) or automatically (i.e. <code>python run_replay --traj_path ...</code>)</li> <li>Add/Update the demonstration to the <code>demonstrations</code> argument.</li> <li>Update <code>demonstration_template</code> to control how the demonstration is displayed to the agent.</li> </ul> </li> </ul> <p>Config files</p> <p>If you're not familiar with how SWE-agent configuration files work, we recommend checking out the <code>config</code> documentation.</p> <p>Next, run your configuration and see how your agent uses the commands! <pre><code>python run.py --config_file config/[your config].yaml ...\n</code></pre></p> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"config/config/","title":"Configuration","text":"<p>This page contains details describing how to write your own configurations to control how agents can interact with the <code>SWEEnv</code> environment.</p> <p>A configuration is represented as a single <code>.yaml</code> file, specified by the <code>--config</code> flag in the command line interface, allowing you to...</p> <ul> <li>Define the commands that agents may use to traverse + modify a codebase (see here for more details)</li> <li>Write prompts that are deterministically/conditionally shown to the agent over the course of a single trajectory.</li> <li>Control the input/output interface that sits between the agent and <code>SWEEnv</code>.</li> </ul> <p>Default config files</p> <p>Our default config files are in the <code>config/</code> directory.</p>"},{"location":"config/config/#configuration-file-fields","title":"Configuration File Fields","text":"<p>The configuration is a <code>.yaml</code> file that consists of several fields. They are fully represented in this following outline:</p> <pre><code># Prompt Templates: Control how observations of environment are shown to agent\nsystem_template: | # .yaml syntax for multi-line string value\n  First `system` message shown to agent\ninstance_template: |- # .yaml syntax for multi-line string value w/ no new line\n  Instance prompt, contains task instance-specific content\nnext_step_template: |-\n  Format template of per-turn observation (Contains standard output from agent's action)\nnext_step_no_output_template: |-\n  Format template of observation when there is no standard output from the agent's action\nformat_error_template: |-\n  Format template of error message (Used when agent's action causes an error)\ndemonstration_template: |\n  Format template for showing a demonstration to the agent\ndemonstrations:\n- `trajectories/&lt;username&gt;/&lt;experiment folder&gt;/*.traj`\n- File is a demonstration of how to solve a task. This could an agent generated trajectory.\n- You can include 1+ demonstrations\n\n# Environment States: Define features of the SWEEnv environment\nenv_variables:\n# Default variables for SWEEnv at the beginning of each instance\n  CURRENT_FILE: 0\n  CURRENT_LINE:\n  OVERLAP:\n  SEARCH_FILES:\n  SEARCH_INDEX:\n  SEARCH_RESULTS:\n  WINDOW_SIZE:\n  START_INDEX:\n  END_INDEX:\n  START_CURSOR:\n  END_CUROSR:\n  START_CURSORS_MARK:\n  END_CURSOR_MARK:\nstate_command: |\n# `state_command` allows you to update state variables to reflect any aspect of the environment (e.g. current working directory)\n  name: state\n  code: |\n    state() { echo '{\"pwd\": \"'$PWD'\"}';\n\n# Action Interface: Define how an agent interacts with the SWEEnv environment\ncommand_files:\n- path/to/bash_file.sh\n- Each file contains a list of commands implemented in bash\n- You can include 1+ command files\nparse_command: Reference to functionality for defining command documentation\nhistory_processor: Reference to functionality for controlling agent's message history\nparse_function: Parser run on agent output\n</code></pre> <p>In the <code>config/</code> directory, we recommend looking at...</p> <ul> <li><code>configs/</code> for examples of properly formatted configuration files. Each configuration differs in its set of commands, input/output format, demonstrations, etc.</li> <li><code>commands/</code> for the bash implementations of the custom commands that SWE-agent uses to navigate + edit the codebase. More information here.</li> </ul> <p>Relative paths</p> <p>Relative paths in config files are resolved to the <code>SWE_AGENT_CONFIG_ROOT</code> environment variable (if set) or the SWE-agent repository root.</p>"},{"location":"config/config/#how-a-configuration-file-is-processed","title":"How a Configuration File is Processed","text":"<p>Some notes on processing that occurs on config fields when SWE-agent is run:</p> <ul> <li>Commands specified in <code>command_files</code> will be parsed into a single block of documentation text that can be referenced as <code>{command_docs}</code>.</li> <li><code>env_variables</code> are the default variables for the bash environment at the beginning of each instance.</li> <li><code>state_command</code> is used to extract state information from the bash environment (formatted as json) to be used in the templates given to the agent.</li> </ul> <p>Possible variables that can be used in templates are: - <code>{command_docs}</code> (an automatically compiled collection of available commands + their docstrings) - any variable given in <code>env_variables</code> (same spelling), e.g., <code>{WINDOW_SIZE}</code> - any variable extracted as json as part of the <code>state_command</code> function - the last observation <code>{observation}</code> - ... this list will grow as we implement more features!</p>"},{"location":"config/config/#template-workflow","title":"Template Workflow","text":"<p>The following diagram illustrates where each template is shown within a single episode of solving one task instance.</p> <p></p> <p>One of three templates can be shown per turn:</p> <ul> <li>\"Next Step\" (<code>next_step_template</code>): Displayed if the model's action successfully runs. The output and a prompt for the next action is shown</li> <li>\"Next Step (No Output)\" (<code>next_step_no_output_template</code>): Displayed if the model's action successfully runs, but does not produce any standard output (e.g. <code>rm</code>, <code>cd</code>)</li> <li>\"Format Error\" (<code>format_error_template</code>): Displayed if the model's response is malformed. Over the next two turns...</li> <li>If one of the model's next response is correct, the message history is updated such that the \"Format Error\" turn is not kept. The episode continues.</li> <li>If the model's next two responses are both malformed, the episode terminates.</li> </ul> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"config/demonstrations/","title":"Changing the demonstrations","text":"<p>An important way to show LMs how to use commands and interact with the environment is through providing a demonstration - which is basically a completed trajectory that the LM can learn from.</p> <p>For simplicity we only ingest demonstrations in the from of a trajectory file. However, since trajectory files are usually JSON, you can convert them to yaml using the <code>make_demos/convert_traj_to_demo.py</code> script to be more human-readable and easier to edit.</p> <p>Demo (yaml) files are stored in the <code>make_demos/demos</code> directory by default and consist primarily of the sequence of actions that an LM would need to take to complete a task. It's important that your demo have the proper format to be parsed by SWE-agent and your config.</p>"},{"location":"config/demonstrations/#converting-an-existing-demonstration","title":"Converting an existing demonstration","text":"<p>Here's how you can make a demo:</p> <ol> <li>Find a basic trajectory that you already like and want to use as the basis for your demo.    For instance, consider the <code>.traj</code> files in the <code>trajectories/demonstrations/</code> folder.</li> <li>Run <code>python convert_traj_to_demo.py &lt;path to trajectory file.traj&gt;</code> to convert the trajectory to a demo.    This demo will be saved as a readable yaml file in the <code>make_demos/demos</code> directory.</li> <li>Edit the demo by hand to make it work for your particular use case and configuration.</li> <li>Run <code>python run_replay.py --traj_path &lt;path to demo&gt; --config_file &lt;path to config file&gt;</code> to execute the actions of the demo, have the system generate the execution output, and ensure that it works as expected.</li> <li>Inspect the resulting trajectory to ensure it was executed correctly.</li> </ol>"},{"location":"config/demonstrations/#manually-creating-a-custom-trajectory","title":"Manually creating a custom trajectory","text":"<p>You can also manually generate a trajectory by running the agent with <code>--model_name human</code> (which allows you to enter the commands that would usually be suggested by the LM) and then convert that trajectory into a demonstration as above. To edit text in <code>human</code> mode:</p> <ol> <li>Run the command <code>edit edit_start_line:edit_end_line</code></li> <li>Write the text you want to insert. Feel free to write the text across multiple lines.</li> <li>Press <code>return</code> then write <code>end_of_edit</code> and then press <code>return</code> again to submit the edit.</li> </ol> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"config/env/","title":"Environment variables","text":"<p>This page details all environment variables that are currently in use by SWE-agent.</p> <ul> <li>All API keys (for LMs and GitHub) can be set as an environment variable. See here for more information.</li> <li>[Experimental] <code>SWE_AGENT_EXPERIMENTAL_COMMUNICATE</code> switches to a faster way of communicating with running docker   shell sessions. This has not been tested on SWE-bench, so it might sometimes break.   However, it can be very useful when running on single issues.</li> <li><code>SWE_AGENT_CONFIG_ROOT</code>: Used to resolve relative paths in the config</li> </ul>"},{"location":"dev/code_structure/","title":"Code structure","text":"<ul> <li>See the <code>scripts/</code> folder for other useful scripts and details.</li> <li>See the <code>config/</code> folder for details about how you can define your own configuration!</li> <li>See the <code>sweagent/agent/</code> folder for details about the logic behind configuration based workflows.</li> <li>See the <code>sweagent/environment/</code> folder for details about the <code>SWEEnv</code> environment (interface + implementation).</li> <li>See the <code>trajectories/</code> folder for details about the output of <code>run.py</code>.</li> <li>See the <code>evaluation/</code> folder for details about how evaluation works.</li> </ul>"},{"location":"dev/contribute/","title":"Contribute to SWE-agent","text":"<p>Formatting change</p> <p>We've recently added automated formatting to our code base. If you are dealing with merge-conflicts when opening a PR or updating your fork, please first install <code>pre-commit</code> and run <code>pre-commit run --all-files</code> and try again.</p> <p>The easiest way to contribute is to give us feedback.</p> <ul> <li>Something isn't working? Open a bug report.   Rule of thumb: If you're running something and you get some error messages, this is the issue type for you.</li> <li>You have a concrete question? Open a question issue.</li> <li>You are missing something? Open a feature request issue</li> <li>Open-ended discussion? Talk on discord. Note that all actionable items should be an issue though.</li> </ul> <p>Wanna do more and actually contribute code? Great! Please see the following sections for tips and guidelines!</p>"},{"location":"dev/contribute/#development-repository-set-up","title":"Development repository set-up","text":"<p>Please install the repository from source, following our usual instructions but add the <code>[dev]</code> option to the <code>pip</code> command (you can just run the command again):</p> <pre><code>pip install -e '.[dev]'\n</code></pre> <p>Then, make sure to set up <code>pre-commit</code>:</p> <pre><code># cd to our repo root\npre-commit install\n</code></pre> <p><code>pre-commit</code> will check for formatting and basic syntax errors before your commits.</p> <p>Autofixes</p> <p>Most problems (including formatting) will be automatically fixed. Therefore, if <code>pre-commit</code>/<code>git commit</code> fails on its first run, simply try running it a second time.</p> <p>Some more autofixes can be enabled with the <code>--unsafe-fixes</code> option from <code>ruff</code>:</p> <pre><code>pipx run ruff check --fix --unsafe-fixes\n</code></pre>"},{"location":"dev/contribute/#running-tests","title":"Running tests","text":"<p>We provide a lot of tests that can be very helpful for rapid development. Run them with</p> <pre><code>pytest\n</code></pre> <p>Some of the tests might be slower than others. You can exclude them with</p> <pre><code>pytest -m \"not slow\"\n</code></pre>"},{"location":"dev/contribute/#tips-for-pull-requests","title":"Tips for pull requests","text":"<ul> <li>If you see a lot of formatting-related merge conflicts, please see below.</li> <li>Please open separate PRs for separate issues. This makes it easier to incorporate part of your changes.</li> <li>It might be good to open an issue and discuss first before investing time on an experimental feature.</li> <li>Don't know where to get started? Look for issues marked \ud83d\udc4b good first issue or \ud83d\ude4f help wanted</li> <li>When changing the behavior of the agent, we need to have some indication that it actually improves the success rate of SWE-agent.   However, if you make the behavior optional without complicating SWE-agent (for example by providing new commands),   we might be less strict.</li> <li>Please add simple unit tests or integration tests wherever possible. Take a look in the tests directory   for inspiration. We emphasize simple easy-tow-rite tests that get a lot of coverage.</li> </ul>"},{"location":"dev/contribute/#merge-conflicts-due-to-formatting","title":"Merge conflicts due to formatting","text":"<p>Information will be added later today.</p>"},{"location":"dev/contribute/#building-the-documentation","title":"Building the documentation","text":"<p>Simply run</p> <pre><code># cd repo root\nmkdocs serve\n</code></pre> <p>and point your browser to port 8000 or click one of the links in the output.</p>"},{"location":"installation/","title":"Setting up SWE-agent","text":"<ul> <li> <p> All in browser</p> <p>Run SWE-agent using GitHub codespaces. All necessary packages will be pre-installedn in an in-browser VSCode environment.</p> <p> Get started</p> </li> <li> <p> Install from source</p> <p>Install SWE-agent from source using <code>pip</code>.</p> <p> Get started</p> </li> <li> <p> Run in docker</p> <p>Pull a docker container and directly run SWE-agent. This is our fallback solution if the local installation does not work for you.</p> <p> Get started</p> </li> </ul>"},{"location":"installation/changelog/","title":"Changelog","text":""},{"location":"installation/changelog/#v050-2024-05-28","title":"v0.5.0 (2024-05-28)","text":"<p>All new commits</p> <p>\u2728 The big news is our brand new documentation \u2728</p> <p>Secondly, @ollmer added a new flag <code>--cache_task_images</code> that will significantly speed up SWE-agent when running on the same environment/repository multiple times (no more waiting for cloning and installation!)</p>"},{"location":"installation/changelog/#breaking-changes","title":"Breaking changes","text":"<ul> <li>Remove direct imports in init.py (you can no longer <code>from sweagent import Agent</code> by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/436</li> <li>We have reformatted our codebase. If you create a PR based on a previous commit, make sure you install our <code>pre-commit</code> hook to avoid merge-conflicts because of formatting. See our docs for more information.</li> </ul>"},{"location":"installation/changelog/#features","title":"Features","text":"<ul> <li>Running the web UI is now supported when running swe-agent completely in docker</li> <li>Speed up evaluation by caching task environments as docker images by @ollmer in https://github.com/princeton-nlp/SWE-agent/pull/317</li> </ul>"},{"location":"installation/changelog/#improved","title":"Improved","text":"<ul> <li>Add gpt-4o model by @raymyers in https://github.com/princeton-nlp/SWE-agent/pull/344</li> <li>Web: Allow to specify commit hash by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/358</li> <li>Add default environment_setup config by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/351</li> <li>Enh: Suppress openai logging; improve formatting of stats by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/416</li> <li>Remove signal dependency by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/428</li> <li>Do not use select if running on Windows by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/429</li> <li>Use custom Config class to support env and keys.cfg (this allows passing keys as environment variables) by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/430</li> </ul>"},{"location":"installation/changelog/#fixes","title":"Fixes","text":"<ul> <li>Web: Fix script_path input by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/334</li> <li>Fix: Don't print patch msg for exit_cost patch by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/343</li> <li>Fix: Do not request job control in bash by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/345</li> <li>Fix: --base_commit not used for gh urls by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/346</li> <li>Fix: Separate data path/traj dir cause exception by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/348</li> <li>Add docker-py lower bound by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/406</li> <li>Fix: IndexError when replaying incomplete trajectories by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/410</li> </ul>"},{"location":"installation/changelog/#040-2024-05-09","title":"0.4.0 (2024-05-09)","text":"<p>All new commits</p>"},{"location":"installation/changelog/#added","title":"Added","text":"<p>We\u2019re excited to launch the SWE-agent web UI! Specify a bug, press start and watch SWE-agent do the magic.</p>"},{"location":"installation/changelog/#030-2024-05-02","title":"0.3.0 (2024-05-02)","text":""},{"location":"installation/changelog/#added_1","title":"Added","text":"<ul> <li>Run SWE-agent in the cloud using GitHub Codespaces</li> <li>Add GPT4-turbo model by @zgrannan in https://github.com/princeton-nlp/SWE-agent/pull/252</li> <li>feat: Amazon Bedrock support (Claude models) by @JGalego in https://github.com/princeton-nlp/SWE-agent/pull/207</li> </ul>"},{"location":"installation/changelog/#fixed","title":"Fixed","text":"<ul> <li>Better error handling for --open_pr by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/239</li> <li>Fixed a potential error by @DanjieTang in https://github.com/princeton-nlp/SWE-agent/pull/242</li> <li>fix: TARGETARCH not set on some OS/docker setups by @mspronesti in https://github.com/princeton-nlp/SWE-agent/pull/249</li> <li>Pass Python version to get_environment_yml by @waterson in https://github.com/princeton-nlp/SWE-agent/pull/271</li> <li>Fix Together model validation error by @mikanfactory in https://github.com/princeton-nlp/SWE-agent/pull/236</li> <li>Doc: Avoid invalid github token by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/292</li> </ul>"},{"location":"installation/changelog/#020-2024-04-15","title":"0.2.0 (2024-04-15)","text":"<p>All new commits</p>"},{"location":"installation/changelog/#added_2","title":"Added","text":"<ul> <li>Allow to run on local repos (new flag: <code>--repo_path</code>) in https://github.com/princeton-nlp/SWE-agent/pull/193</li> <li>Patch files are now saved separately to a patch directory in https://github.com/princeton-nlp/SWE-agent/pull/126</li> <li>Allow to supply custom installation commands when running on gh issues or locally (<code>--environment_setup</code>) in https://github.com/princeton-nlp/SWE-agent/pull/153</li> <li>Allow to specify openapi base url in <code>keys.cfg</code> in https://github.com/princeton-nlp/SWE-agent/pull/118</li> </ul>"},{"location":"installation/changelog/#improved_1","title":"Improved","text":"<ul> <li>Improve error handling of docker issues in https://github.com/princeton-nlp/SWE-agent/pull/165</li> <li>Make github token fully optional in https://github.com/princeton-nlp/SWE-agent/pull/189</li> </ul>"},{"location":"installation/changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Fix opening PR from fork in https://github.com/princeton-nlp/SWE-agent/pull/229</li> <li>Fix: Choosing TogetherAI models in https://github.com/princeton-nlp/SWE-agent/pull/130</li> </ul>"},{"location":"installation/codespaces/","title":"Running SWE-agent in your browser","text":"<p>Running SWE-agent in your browser is the easiest way to try out our project.</p> <ol> <li>Click </li> <li>Add your API keys to <code>keys.cfg</code> (find the file in the left sidebar and fill out the template). More information on the keys here.</li> <li>Make sure to wait until the <code>postCreateCommand</code> in the terminal window at the bottom is finished</li> <li>Enter your SWE-agent command, see using the web interface or using the command line.</li> </ol>"},{"location":"installation/codespaces/#running-the-web-ui","title":"Running the Web UI","text":"<p>Go to the terminal and enter</p> <pre><code>./start_web_ui.sh\n</code></pre> <p>After a while, you should see a popup offering you to forward port <code>3000</code>. Click <code>Open in Browser</code>.</p> <p></p> <p>If you instead only see the offer to forward port <code>8000</code>, do not click it (this is the port that's being used by the backend).</p> <p>Instead, click on the <code>Ports</code> tab, and click on the globe next to port <code>3000</code>:</p> <p></p> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"installation/docker/","title":"Fallback: Usage with docker","text":"<p>Instead of installing SWE-agent from source, you can also run the software directly using Docker.</p> <ol> <li>Install Docker, then start Docker locally.</li> <li>Run <code>docker pull sweagent/swe-agent:latest</code></li> <li>Add your API tokens to a file <code>keys.cfg</code> as explained here or pass them as    environment variables.</li> </ol> <p>Assuming that you create <code>keys.cfg</code> in the current directory, run</p> <pre><code>docker run --rm -it -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v $(pwd)/keys.cfg:/app/keys.cfg \\\n  sweagent/swe-agent-run:latest \\\n  python run.py --image_name=sweagent/swe-agent:latest \\\n  --model_name gpt4 \\\n  --data_path https://github.com/pvlib/pvlib-python/issues/1603 \\\n  --config_file config/default_from_url.yaml  --skip_existing=False\n</code></pre> <p>Windows</p> <p>If you're using docker on Windows, use <code>-v //var/run/docker.sock:/var/run/docker.sock</code> (double slash) to escape it (more information).</p> <p>If you instead want to pass the keys as environment variables, use</p> <pre><code>docker run --rm -it -v /var/run/docker.sock:/var/run/docker.sock \\\n  -e GITHUB_TOKEN=\"yourgithubtoken\" \\\n  -e OPENAI_API_KEY=\"youropenaikey\" \\\n  sweagent/swe-agent-run:latest \\\n  # rest of the command above\n</code></pre> <p>Getting updates</p> <p>Even though the image <code>sweagent/swe-agent:latest</code> has the tag <code>latest</code>, it is not automatically updated every time you run <code>docker run</code>. Instead, you need to manually run <code>docker pull sweagent/swe-agent:latest</code> periodically.</p> <p>Retrieving generated files</p> <p>The optional <code>--rm</code> flag removes the docker container after the command has terminated. Therefore, to retrieve files (like generated patch files) from the container, please remove this flag.</p> <p>To run the web server, make sure to forward port 3000:</p> <pre><code>docker run -p 3000:3000 -it -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v $(pwd)/keys.cfg:/app/keys.cfg \\\n  sweagent/swe-agent-run:latest bash start_web_ui.sh\n</code></pre> <p>More tips</p> <p>See the installation issues section for more help if you run into trouble.</p> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"installation/keys/","title":"Adding your API keys","text":"<p>In order to access the LM of your choice (and to access private GitHub repositories), you need to supply the corresponding keys.</p> <p>There are two options to do this:</p> <ol> <li>Set the corresponding environment variables.</li> <li>Create a <code>keys.cfg</code> file at the root of this repository.</li> </ol> <p>The following <code>keys.cfg</code> example shows you how the keys are named:</p> <pre><code># Remove the comment '#' in front of the line for all keys that you have set\n# GITHUB_TOKEN: 'GitHub Token for access to private repos'\n# OPENAI_API_KEY: 'OpenAI API Key Here if using OpenAI Model'\n# ANTHROPIC_API_KEY: 'Anthropic API Key Here if using Anthropic Model'\n# TOGETHER_API_KEY: 'Together API Key Here if using Together Model'\n# AZURE_OPENAI_API_KEY: 'Azure OpenAI API Key Here if using Azure OpenAI Model'\n# AZURE_OPENAI_ENDPOINT: 'Azure OpenAI Endpoint Here if using Azure OpenAI Model'\n# AZURE_OPENAI_DEPLOYMENT: 'Azure OpenAI Deployment Here if using Azure OpenAI Model'\n# AZURE_OPENAI_API_VERSION: 'Azure OpenAI API Version Here if using Azure OpenAI Model'\n# OPENAI_API_BASE_URL: 'LM base URL here if using Local or alternative api Endpoint'\n</code></pre> <p>See the following links for tutorials on obtaining Anthropic, OpenAI, and Github tokens.</p>"},{"location":"installation/source/","title":"Installation from source","text":"<p>Installation from source is the preferred way to set up SWE-agent on your machine.</p> <p>Issues on Windows</p> <p>Expect some issues with Windows (we're working on them). In the meantime, use Docker (see below).</p> <ol> <li>Install Docker, then start Docker locally.</li> <li>If you plan on using the web-based GUI: Install <code>nodejs</code>.</li> <li>Clone the repository (<code>git clone https://github.com/princeton-nlp/SWE-agent.git</code>)</li> <li>Run <code>pip install --editable .</code> at the repository root (as with any python setup, it's recommended to use conda or virtual environments to manage dependencies). Error about editable install? Please update pip<sup>1</sup>.</li> <li>Run <code>./setup.sh</code> to create the <code>swe-agent</code> docker image.</li> <li>Create a <code>keys.cfg</code> file at the root of this repository (more information).</li> </ol> <p>Docker issues</p> <p>If you run into docker issues, see the installation tips section for more help.</p> <p>Updating</p> <p>SWE-agent is still in active development. Features and enhancement are added often. To make sure you are on the latest version, periodically run <code>git pull</code> (there is no need to redo the <code>pip install</code>).</p> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul> <ol> <li> <p>You can update <code>pip</code> with <code>pip install --upgrade pip</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"installation/tips/","title":"More installation tips","text":"<p>If you seem to be having issues with running docker</p> <ul> <li>Make sure that you allow the use of the Docker socket. In Docker desktop, click Settings &gt; Advanced &gt; Allow the default Docker socket to be used (requires password)</li> <li>If your docker installation uses a different socket, you might have to symlink them, see this command for example</li> <li>If you are using any containers from dockerhub (i.e., you ran <code>docker pull ...</code> or you are running <code>docker run ...</code>), please make sure that you are using the latest   versions. Just because an image has the <code>latest</code> tag (e.g., <code>sweagent/swe-agent-run:latest</code>) does not mean that it will auto-update. Please run   <code>docker pull sweagent/swe-agent-run:latest</code> to make sure you actually have the most recent version!</li> </ul> <p>Any remaining issues? Please open a GitHub issue!</p>"},{"location":"usage/","title":"Usage","text":"<p>We currently provide two interfaces to SWE-agent:</p> <ul> <li> <p> Command line interface (CLI)</p> <p>The default way of running SWE-agent with maximum options.</p> <p> Get started</p> </li> <li> <p> Graphical user interface</p> <p>We provide a browser-based graphical user interface particularly optimized for developers wanting to use SWE-agent as a tool.</p> <p> Get started</p> </li> </ul>"},{"location":"usage/benchmarking/","title":"Benchmarking","text":"<p>There are two steps to the SWE-agent pipeline. First SWE-agent takes an input GitHub issue and returns a pull request that attempts to fix it. We call that step inference. The second step (currently, only available for issues in the SWE-bench benchmark) is to evaluate the pull request to verify that it has indeed fixed the issue.</p> <p>Architectures</p> <p>At this moment, there are known issues with a small number of repositories that don't install properly for <code>arm64</code> / <code>aarch64</code> architecture computers. We're working on a fix, but if you'd like to run and evaluate on the entirety of SWE-bench, the easiest way is by using an <code>x86</code> machine.</p>"},{"location":"usage/benchmarking/#inference","title":"\ud83d\udc69\u200d\ud83d\udcbb Inference","text":"<p>Run SWE-agent on SWE-bench Lite and generate patches.</p> <pre><code>python run.py --model_name gpt4 \\\n  --per_instance_cost_limit 2.00 \\\n  --config_file ./config/default.yaml\n</code></pre> <p>If you'd like to run on a single issue from SWE-bench, use the <code>--instance_filter</code> option as follows: <pre><code>python run.py --model_name gpt4 \\\n  --instance_filter marshmallow-code__marshmallow-1359\n</code></pre></p>"},{"location":"usage/benchmarking/#evaluation","title":"\ud83e\uddea Evaluation","text":"<p>The <code>evaluation/</code> folder provides SWE-agent compatible scripts for running SWE-bench style evaluation on model patch predictions. In addition, we also include additional scripts to quantify model performance on \"subtasks\" within the SWE-bench task, such as identifying the right file(s) to edit.</p>"},{"location":"usage/benchmarking/#quick-start","title":"\ud83d\udc07 Quick Start","text":"<p>You can run evaluations on SWE-bench by passing in the predictions generated by SWE-agent (usually named <code>all_preds.jsonl</code>). Simply run the following script:</p> <pre><code>./run_eval.sh &lt;path to predictions&gt;\n</code></pre> <p>The <code>&lt;predictions_path&gt;</code> arguments should look like</p> <pre><code>../trajectories/&lt;username&gt;/&lt;model&gt;-&lt;dataset&gt;-&lt;hyperparams&gt;/all_preds.jsonl\n</code></pre> <p>Depending on the number of task instances and how long setting up the execution environment takes, the evaluation could take a couple minutes or to 7 hours for the entirety of the SWE-bench test split.</p> <p>When evaluation finishes, you should see an output similar to the following: <pre><code>2024-03-31 16:47:00,263 - taskenv_context_manager - INFO - [pvlib__pvlib-python__0.8] [pvlib__pvlib-python-1395] Installing with command: . /n/fs/p-swe-bench/testbed/ba397fe0d6/pvlib__pvlib-python/0.8/tmpom22t9na/miniconda3/bin/activate pvlib__pvlib-python__0.8 &amp;&amp; echo 'activate successful' &amp;&amp; pip install -e .[all]\n2024-03-31 16:47:10,602 - taskenv_context_manager - INFO - [pvlib__pvlib-python__0.8] [pvlib__pvlib-python-1395] Installation successful\n2024-03-31 16:47:10,619 - taskenv_context_manager - INFO - [pvlib__pvlib-python__0.8] [pvlib__pvlib-python-1395] Apply patch successful (test)\n2024-03-31 16:47:10,635 - taskenv_context_manager - INFO - [pvlib__pvlib-python__0.8] [pvlib__pvlib-python-1395] Apply patch successful (pred)\n2024-03-31 16:47:13,453 - taskenv_context_manager - INFO - [pvlib__pvlib-python__0.8] [pvlib__pvlib-python-1395] Test script run successful\n==================================\nLog directory for evaluation run: /n/fs/p-swe-bench/results/gpt-4-1106-preview__swe-bench-dev-40-seed24__default_sys-env_window100-detailed_cmd_format-full_history-1_demos__t-0.20__p-0.95__c-4.00__install-1__sweep-01-run-4\n== Evaluation Report ==\n{'# Not Generated': 1, '# Generated': 36, '# Applied': 34, '# Resolved': 5}\n- Wrote per-instance scorecards to /&lt;path to SWE-agent&gt;/trajectories/carlosejimenez/gpt-4-1106-preview__swe-bench-dev-40-seed24__default_sys-env_window100-detailed_cmd_format-full_history-1_demos__t-0.20__p-0.95__c-4.00__install-1__sweep-01-run-4/scorecards.json\n- Wrote summary of run to /&lt;path to SWE-agent&gt;/trajectories/carlosejimenez/gpt-4-1106-preview__swe-bench-dev-40-seed24__default_sys-env_window100-detailed_cmd_format-full_history-1_demos__t-0.20__p-0.95__c-4.00__install-1__sweep-01-run-4/results.json\nReference Report:\n{'# Not Generated': 1, '# Generated': 36, '# Applied': 34, '# Resolved': 5}\n</code></pre></p>"},{"location":"usage/benchmarking/#swe-bench-evaluation","title":"\ud83e\ude91 SWE-bench Evaluation","text":"<p><code>evaluation.py</code>: This script contains the logic for SWE-bench evaluation adapted for the SWE-agent setting. Given a set of predictions (e.g. <code>trajectories/&lt;user&gt;/&lt;experiment&gt;/all_preds.jsonl</code>), we...</p> <ol> <li>Filter + analyze predictions.</li> <li>Run SWE-bench style execution based evaluation.</li> <li>Save outcomes to <code>results.json</code> and <code>scorecards.json</code> files with info about task-specific and overall performance.</li> </ol> <p>Examples</p> <p><code>run_eval.sh</code> (see above) is provided as an example of how to run <code>evaluation.py</code></p> <p>Arguments:</p> <ul> <li><code>--predictions_path (required)</code>: The path to the file containing predictions (.jsonl format). This file includes the predictions that need to be evaluated against the benchmark tasks.</li> <li><code>--log_dir (required)</code>: The directory path where log files related to the evaluation process will be stored. It's used for saving logs that are generated during the evaluation.</li> <li><code>--swe_bench_tasks (required)</code>: The path to the file containing the SWE-bench task instances. This file includes the details of the tasks against which the predictions will be evaluated.</li> <li><code>--testbed (required)</code>: The directory path for the testbed, which is likely used for setting up the environment or context for the evaluations.</li> <li><code>--skip_existing (optional)</code>: If specified, the script will skip over log files that already exist, preventing re-evaluation of those tasks.</li> <li><code>--timeout (optional)</code>: Specifies the timeout in seconds for the evaluation process (default is 900 seconds). This helps in controlling the duration of each evaluation task to avoid excessively long running times.</li> <li><code>--verbose (optional)</code>: Enables verbose mode, which will provide more detailed output during the script execution. This is useful for debugging or getting more insight into the process.</li> <li><code>--conda_link (optional)</code>: Allows specifying a URL to a Conda installation that should be used for the evaluation environment. This can be necessary if the evaluation requires a specific software environment.</li> <li><code>--log_suffix (optional)</code>: An additional parameter to specify a suffix for log files. This can be used for organizing logs more effectively, especially when running multiple evaluations in parallel or under different configurations.</li> </ul>"},{"location":"usage/benchmarking/#viewing-results","title":"\ud83d\udcc8 Viewing Results","text":"<p><code>aggregate_results.py</code>: This script aggregates and displays experiment results from the <code>trajectories/</code> folder.</p> <ul> <li>Experiments are grouped by <code>(Model, Dataset, Config File, Temp., Top P, Cost, Install)</code>.</li> <li>The following statistics for each experiment run are shown:<ul> <li><code>Not Generated</code>: # of task instances with no patch generated</li> <li><code>Generated</code>: # of task instances with patch</li> <li><code>Applied</code>: # of patches that applied successfully</li> <li><code>Resolved</code>: # of task instances resolved</li> <li><code>Costs [Success|Failed|Overall]</code>: Cost of [successful|failed|any] run</li> </ul> </li> <li>If there are multiple runs of an experiment (distinguished by <code>--suffix run&lt;i&gt;</code>), the above statistics are aggregate as totals or means.</li> </ul> <p>Usage:</p> <pre><code>python aggregate_results.py\n</code></pre> <p>Arguments:</p> <ul> <li><code>--folder (type: str, default: ../trajectories)</code>: Specifies the folder containing the experiment * results. This is where the script will look to gather data.</li> <li><code>--model (type: str, nargs: '+')</code>: Filters the results by model(s). Only results corresponding to the * specified model(s) will be included.</li> <li><code>--dataset (type: str, nargs: '+')</code>: Filters the results by dataset(s). Only results for the specified * dataset(s) will be analyzed.</li> <li><code>--setup (type: str, nargs: '+')</code>: Filters the results by setup(s). This allows focusing on specific * experiment configurations.</li> <li><code>--runs_min (type: int)</code>: The minimum number of runs an experiment should have to be included in the * analysis. Helps exclude experiments with insufficient data.</li> <li><code>--runs_max (type: int)</code>: The maximum number of runs to consider for each experiment. This can limit the data to the most relevant runs.</li> </ul> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"usage/cl_tutorial/","title":"Command line usage tutorial","text":"<p>This tutorial walks you trough running SWE-agent from the command line. Beginners might also be interested in the our web-based GUI (see here). This tutorial focuses on using SWE-agent as a tool to solve individual issues. Benchmarking SWE-agent is covered separately.</p>"},{"location":"usage/cl_tutorial/#getting-started","title":"Getting started","text":"<p>For the CLI, use the <code>run.py</code> script. Let's start with an absolutely trivial example and solve an issue about a simple syntax error (<code>swe-agent/test-repo #1</code>)</p> <pre><code>python run.py \\\n  --model_name gpt4 \\\n  --data_path https://github.com/SWE-agent/test-repo/issues/1 \\\n  --config_file config/default_from_url.yaml \\\n  --per_instance_cost_limit 2.00\n</code></pre> <p>Here,</p> <ul> <li><code>--model_name</code> sets the language model that is used by SWE-agent (with <code>gpt4</code> being the default). More information on the available models in our FAQ</li> <li><code>--data_path</code> points to the source of the problem statement (for example, the GitHub issue that you want to solve). You can also point it to local files (see below)</li> <li><code>--config_file</code> includes settings such as the prompts. Changing the config file is the easiest way to get started with modifying SWE-agent (more advanced options are discussed here).</li> <li><code>--per_instance_cost_limit</code> limits the total inference cost to $2 (default is $3).</li> </ul> <p>All options</p> <p>Run <code>python run.py --help</code> to see all available options for <code>run.py</code>. This tutorial will only cover a subset of options.</p> <p>Running more than once</p> <ul> <li>The complete details of the run are saved as a \"trajectory\" file (more about them here). They can also be turned into new demonstrations.</li> <li>If you run the same command more than once, you will find that SWE-agent aborts with <code>Skipping existing trajectory</code>. You can either remove the trajectory from the warning message, or add the <code>--skip_existing=False</code> flag.</li> <li>If you solve multiple issues from the same repository/in the same environment, you can specify the   <code>--cache_task_images</code> flag. This will create a persistent docker image with the initialized environment   required for the problem.</li> </ul>"},{"location":"usage/cl_tutorial/#specifying-the-repository","title":"Specifying the repository","text":"<p>In the above example, the repository/codebase is inferred from the <code>--data_path</code>. This options is currently only available for GitHub issues. For all other use cases, you can specify <code>--repo_path</code>, which accepts either GitHub URLs or paths to local repositories.</p> <p>To try it out, let's clone the test repository from the previous section.</p> <pre><code>git clone git@github.com:SWE-agent/test-repo.git\n</code></pre> <p>and then run</p> <pre><code>python run.py \\\n  --data_path /path/to/test-repo/problem_statements/1.md \\\n  --repo_path /path/to/test-repo \\\n  --config_file config/default_from_url.yaml \\\n  --apply_patch_locally\n</code></pre> <p>where you replaced paths with the prefix <code>/path/to/.../</code> with the actual paths to the corresponding file/directory.</p> <p>We have also added a new flag, <code>--apply_patch_locally</code>, which will make SWE-agent apply the changes to the local repository (if it believes that it has successfully solved the issue).</p> <p>You can mix and match the different ways of specifying problem statements and repositories. For example, any of the following combination of options also works</p> <ul> <li>Local problem statement with GitHub repository (<code>--data_path /path/to/problem.md --repo_path https://github.com/...</code>): Let SWE-agent work on something that wasn't reported yet</li> <li>GitHub issue with local repository (<code>--data_path https://github.com/.../issues/.. --repo_path /path/to/... --apply_patch_locally</code>): Let SWE-agent solve a GitHub issue locally (for example to edit the solution afterwards)</li> <li>GitHub issue with different GitHub repository: Useful with the <code>--open_pr</code> flag (see below) when working from a fork.</li> </ul> <p>In addition, if <code>--repo_path</code> points to a GitHub repository, you can use <code>--base_commit</code> to specify</p> <ul> <li>A branch name (e.g., <code>dev</code>),</li> <li>A tag (e.g., <code>v1.0.0</code>),</li> <li>A commit hash (e.g., <code>a4464baca1f28d7733337df6e4daa6c1ed920336</code>).</li> </ul> <p>SWE-agent will then start from this commit when trying to solve the problem.</p> <p>Uncommitted changes</p> <p>When running with a local <code>--repo_path</code>, SWE-agent will use the last commit, i.e., all local, uncommitted changes will not be seen by SWE-agent.</p>"},{"location":"usage/cl_tutorial/#installing-dependencies-and-setting-up-the-environment","title":"Installing dependencies and setting up the environment","text":"<p>Now let's move on to a slightly more complicated issue (<code>swe-agent/test-repo #22</code>).</p> <p>What makes it more complicated? This time the problematic code is part of a library <code>testpkg</code>, so SWE-agent first has to install the package in order to reproduce the issue before searching for the problematic code.</p> <p>In most circumstances, GPT4 will attempt to install the package and requirements (usually with some form of <code>pip install .</code> or <code>pip install pkg</code>). However, this wastes valuable queries to the LM. In addition, you might need to run your software for a specific python version or have other specific environment settings. The <code>--environment_setup</code> flag is used to fix this problem.</p> <p>Let's try it:</p> <pre><code>python run.py \\\n  --data_path https://github.com/SWE-agent/test-repo/issues/22 \\\n  --config_file config/default_from_url.yaml \\\n  --environment_setup config/environment_setup/py310_default.yaml\n</code></pre> <p>This time, <code>pip install -e .</code> is called before SWE-agent gets to work, installing the package defined in the repository.</p> <p>Let's take a look at the config file</p> <pre><code>python: '3.10'\ninstall: 'pip install -e .'\n</code></pre> <p>Here, <code>install</code> is an arbitrary command that is run, while <code>python</code> will be the python version that is setup with conda.</p> <p>The config file also provides two more top level directives:</p> <ul> <li><code>packages</code>: Path to a <code>requirements.txt</code> or to a <code>env.yml</code> as readable by conda</li> <li><code>pip_packages</code>: A list of python packages that are installed with <code>pip install PACKAGE</code></li> </ul>"},{"location":"usage/cl_tutorial/#taking-actions","title":"Taking actions","text":"<ul> <li>As mentioned above, you can use <code>--apply_patch_locally</code> to have SWE-agent apply successful solution attempts to local files.</li> <li>Alternatively, when running on a GitHub issue, you can have the agent automatically open a PR if the issue has been solved by supplying the <code>--open_pr</code> flag.   Please use this feature responsibly (on your own repositories or after careful consideration).</li> </ul> <p>Alternatively, you can always retrieve the patch that was generated by SWE-agent. Watch out for the followoing message in the log:</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udf89 Submission successful \ud83c\udf89 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 SWE-agent has produced a patch that it believes will solve the issue you submitted! \u2502\n\u2502 Use the code snippet below to inspect or apply it!                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>And follow the instructions below it:</p> <pre><code> # The patch has been saved to your local filesystem at:\n PATCH_FILE_PATH='/Users/.../patches/05917d.patch'\n # Inspect it:\n cat \"${PATCH_FILE_PATH}\"\n # Apply it to a local repository:\n cd &lt;your local repo root&gt;\n git apply \"${PATCH_FILE_PATH}\"\n</code></pre> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"usage/faq/","title":"FAQ","text":""},{"location":"usage/faq/#what-models-are-supported","title":"What models are supported?","text":"<p>Models are configured in <code>models.py</code> (we're working on giving a complete list of model settings).</p> <p>Here are some few examples:</p> <pre><code>gpt4\ngpt4o\ngpt4-turbo\nclaude-2\nclaude-opus\nclaude-sonnet\nclaude-haiku\n</code></pre>"},{"location":"usage/faq/#ollama-support","title":"Ollama support","text":"<p>Models served with an ollama server can be used by specifying <code>--model</code> with <code>ollama:model_name</code> and <code>--host_url</code> to point to the url used to serve ollama (<code>http://localhost:11434</code> by default). See more details about using ollama here.</p> <pre><code>python run.py --model_name ollama:deepseek-coder:6.7b-instruct \\\n  --host_url http://localhost:11434 \\\n  --data_path https://github.com/pvlib/pvlib-python/issues/1603 \\\n  --config_file config/default_from_url.yaml\n</code></pre>"},{"location":"usage/faq/#models-for-testing","title":"Models for testing","text":"<p>We also provide models for testing SWE-agent without spending any credits</p> <ul> <li><code>HumanModel</code> and <code>HumandThoughtModel</code> will prompt for input from the user that stands in for the output of the LM. This can be used to create new demonstrations.</li> <li><code>ReplayModel</code> takes a trajectory as input and \"replays it\"</li> <li><code>InstantEmptySubmitTestModel</code> will create an empty <code>reproduce.py</code> and then submit</li> </ul> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"usage/inspector/","title":"Trajectory inspector","text":"<p>We provide a web interface for visualizing <code>.traj</code> files from the <code>trajectories</code> folder more easily.</p> <p>Set Up</p> <ul> <li>Change to the <code>inspector</code> directory</li> <li>Run <code>python server.py --directory insert_full_absolute_path_to_the_trajectories_folder_here/trajectories</code></li> <li>Open http://localhost:8000 in your browser to use the inspector.</li> </ul> <p>Additional flags</p> <ul> <li><code>--data_path</code>: Path to SWE-bench style dataset that trajectories were generated for (Optional)</li> <li><code>--directory</code>: Directory of trajectories to inspect (Defaults to <code>./trajectories</code> folder)</li> <li><code>--port</code>: Port to host web app (Defaults to <code>8000</code>).</li> </ul> <p>Example Usage</p> <p>From running the command:</p> <p><pre><code>python server.py --directory /Users/ofirp/swe-agent/trajectories\n</code></pre> The inspector will then be launched in the browser:</p> <p></p> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"usage/trajectories/","title":"Trajectories","text":"<p>The <code>trajectories/</code> folder is the default location that experiment results (invocations of <code>run.py</code>) will be written to.</p> <p>At a high level, the experiments folder is organized in the following manner: <pre><code>trajectories\n\u251c\u2500\u2500 &lt;user 1&gt; \ud83d\udc69\u200d\ud83d\udcbb\n\u2502 \u251c\u2500\u2500 &lt;experiment 1&gt; \ud83e\uddea\n\u2502 \u2502 \u251c\u2500\u2500 all_preds.jsonl\n\u2502 \u2502 \u251c\u2500\u2500 args.yaml\n\u2502 \u2502 \u251c\u2500\u2500 *.html (Webpage Files)\n\u2502 \u2502 \u2514\u2500\u2500 *.traj (Trajectories)\n\u2502 \u2514\u2500\u2500 &lt;experiment 2&gt; \ud83e\uddea\n\u2502   \u251c\u2500\u2500 all_preds.jsonl\n\u2502   \u251c\u2500\u2500 args.yaml\n\u2502   \u251c\u2500\u2500 *.html (Webpage Files)\n\u2502   \u2514\u2500\u2500 *.traj (Trajectories)\n\u251c\u2500\u2500 &lt;user 2&gt; \ud83d\udc68\u200d\ud83d\udcbb\n\u2502 \u251c\u2500\u2500 &lt;experiment 1&gt; \ud83e\uddea\n\u2502 \u2502 \u2514\u2500\u2500 ...\n\u2502 \u2514\u2500\u2500 &lt;experiment 2&gt; \ud83e\uddea\n\u2502   \u2514\u2500\u2500 ...\n...\n</code></pre> Where every experiment follows the pattern <code>trajectories/&lt;user name&gt;/&lt;experiment name&gt;</code>. The <code>&lt;user name&gt;</code> is automatically inferred from your system, and the <code>experiment name</code> is inferred from the arguments of the <code>run.py</code>.</p> <p>Viewing trajectories</p> <p>We provide a trajectory viewer for an easy viewing of trajectories.</p>"},{"location":"usage/trajectories/#how-an-experiment-folder-is-generated","title":"How an Experiment Folder is Generated","text":"<p>Each call to <code>run.py</code> produces a single <code>trajectories/&lt;user name&gt;/&lt;experiment name&gt;</code> folder containing the following assets:</p> <ul> <li><code>all_preds.jsonl</code>: A single file containing all of the predictions generated for the experiment (1 prediction per task instance), where each line is formatted as: <pre><code>{\n    \"instance_id\": \"&lt;Unique task instance ID&gt;\",\n    \"model_patch\": \"&lt;.patch file content string&gt;\",\n    \"model_name_or_path\": \"&lt;Model name here (Inferred from experiment configs)&gt;\",\n}\n</code></pre></li> <li><code>args.yaml</code>: A summary of the configurations for the experiment run.</li> <li><code>&lt;instance_id&gt;.traj</code>: A <code>.json</code> formatted file containing the (thought, action, observation) turns generated by SWE-agent towards solving <code>&lt;instance_id&gt;</code>.</li> <li><code>&lt;instance_id&gt;.html</code>: An <code>.html</code> single webpage render of the trajectory, which can be directly opened in the browser for easier viewing of the trajectory.</li> </ul> <p>Tip</p> <ul> <li>Evaluation is not completed by <code>run.py</code>, it is a separate step (see benchmarking)</li> <li><code>all_preds.jsonl</code> can be referenced directly into <code>evaluation/run_eval.sh</code> to run evaluation (see benchmarking)</li> <li>Trajectories can be turned into custom demonstrations for SWE-agent (more information).</li> </ul> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"usage/web_ui/","title":"Using the web interface","text":"<p>Our graphical web interface is optimized for using SWE-agent as a developer tool, fixing single GitHub issues or working in local repositories. However, it is still missing some of the options of the command line interface.</p>"},{"location":"usage/web_ui/#quickstart","title":"Quickstart","text":"<p>To start our web UI, simply run</p> <pre><code>./start_web_ui.sh\n</code></pre> <p>from the root of the repository.</p> <p>Opening the webpage</p> <p>If the user interface doesn't automatically open in your browser, please open it at <code>http://localhost:3000</code>. Running from GitHub codespaces? More tips here.</p> <p>Running from Docker</p> <p>If you run SWE-agent from the <code>docker-run</code> Docker container, please see here for how to start the web server.</p>"},{"location":"usage/web_ui/#manually-starting-frontend-and-backend","title":"Manually starting frontend and backend","text":"<p>The web UI consists of a frontend written in react (showing the pretty control elements) and a backend written with flask. The <code>./start_web_ui.sh</code> starts both of them in the background. However, this might not be best for development and debugging. This section explains how to start both parts separately.</p> <p>First, let's start the backend:</p> <pre><code>python sweagent/api/server.py\n</code></pre> <p>You should see output similar to the following:</p> <pre><code> * Serving Flask app 'server'\n * Debug mode: on\n2024-05-23 11:30:45,436 - werkzeug - INFO - WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n * Running on http://127.0.0.1:8000\n2024-05-23 11:30:45,437 - werkzeug - INFO - Press CTRL+C to quit\n2024-05-23 11:30:45,437 - werkzeug - INFO -  * Restarting with watchdog (fsevents)\n2024-05-23 11:30:46,484 - werkzeug - WARNING -  * Debugger is active!\n2024-05-23 11:30:46,492 - werkzeug - INFO -  * Debugger PIN: 123-594-933\n</code></pre> <p>Port availability</p> <p>If see an error about port 8000 not being available, please first close any application that occupies it. The frontend currently expects the <code>flask</code> server on port 8000, so choosing a different port won't work.</p> <p>Now, open a new terminal tab and navigate to the <code>frontend</code> directory:</p> <pre><code>cd sweagent/frontend\n</code></pre> <p>First, let's install the react dependencies:</p> <pre><code>npm install\n</code></pre> <p>And start the server:</p> <pre><code>npm start\n</code></pre> <p>This should also open the corresponding page in your browser. If not, check with the tips above. The default port that is being served is port 3000.</p> <p>Possible errors</p> <p>If you see errors</p> <pre><code>Proxy error: Could not proxy request /socket.io/?EIO=4&amp;transport=polling&amp;t=O-c5kv9 from localhost:3000 to http://localhost:8000.\nSee https://nodejs.org/api/errors.html#errors_common_system_errors for more information (ECONNREFUSED).\n</code></pre> <p>something went wrong with the backend part.</p> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"}]}